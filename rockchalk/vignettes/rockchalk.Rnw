%% LyX 2.0.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[american,noae]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{url}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\usepackage{Sweavel}
<<echo=F>>=
  if(exists(".orig.enc")) options(encoding = .orig.enc)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\VignetteIndexEntry{Using rockchalk}

\usepackage{Sweavel}
\usepackage{graphicx}
\usepackage{color}

\usepackage[samesize]{cancel}



\usepackage{ifthen}

\makeatletter

\renewenvironment{figure}[1][]{%

 \ifthenelse{\equal{#1}{}}{%

   \@float{figure}

 }{%

   \@float{figure}[#1]%

 }%

 \centering

}{%

 \end@float

}

\renewenvironment{table}[1][]{%

 \ifthenelse{\equal{#1}{}}{%

   \@float{table}

 }{%

   \@float{table}[#1]%

 }%

 \centering

%  \setlength{\@tempdima}{\abovecaptionskip}%

%  \setlength{\abovecaptionskip}{\belowcaptionskip}%

% \setlength{\belowcaptionskip}{\@tempdima}%

}{%

 \end@float

}


%\usepackage{listings}
% Make ordinary listings look as if they come from Sweave
\lstset{tabsize=2, breaklines=true,style=Rstyle}

% In document Latex options:
\fvset{listparameters={\setlength{\topsep}{0em}}}
\def\Sweavesize{\scriptsize} 
\def\Rcolor{\color{black}} 
\def\Rbackground{\color[gray]{0.90}}

\makeatother

\usepackage{babel}
\begin{document}

\title{Using rockchalk for Regression Analysis}


\author{Paul Johnson}

\maketitle
% In document Latex options:
\fvset{listparameters={\setlength{\topsep}{0em}}}
\SweaveOpts{ae=F,nogin=T}

<<Roptions, echo=F>>=
options(device = pdf)
options(width=80, prompt=" ", continue="  ")
options(useFancyQuotes = FALSE) 
@

In case you want to be involved in development, the source code is
available on GitHub. Please browse http://github.com/pauljohn32. The
read-only git archive address is \url{git://github.com/pauljohn32/rockchalk.git}.
Once you learn how to ``clone'' the repository with a git client,
then we can discuss ways for you to contribute.


\section{Introduction}

This is the Spring, 2013 update of the rockchalk package. I offer
a course in regression analysis for social and behavioral scientists
every year. As the course goes on, I keep track of the difficulties
that the students experience with R and I craft functions that facilitate
their work on particular assignments. In this edition, I offer several
enhancements in the calculation of predicted values, regression plots,
and regression tables. 

The core of the effort this time has been development of an internally
coherent framework that allows students to create ``newdata'' objects
in a flexible and convenient way. Users are welcome to use the newdata()
function directly, but I offer a convenience called predictOMatic()
that allows one to specify a regression model and then receive predicted
values for a range of various input values. The default output will
be similar to other regression software tools that provide ``marginal
effect'' estimates for regression analysis. 

In this edition of rockchalk, I have exerted a great deal of effort
to improve the consistency of predicted value plots for regression
analysis. The plotSlopes() function is a plotter for linear models
with interaction terms. It draws a plot on the screen, but it also
creates an output object that can be subjected to a follow-up hypothesis
test (by the testSlopes() function). In version 1.8, there is a new
plot method for testSlopes objects that employ interactions of numeric
predictors. The plots for testSlopes objects will be the most useful
new feature in this version of the package. 

For people who want to plot nonlinear regressions, the function plotCurves()
is a replacement for plotSlopes. It can handle any sort of nonlinearity
and interaction on the right hand side. Where some regression formula
can cause R's termplot() function to fail, plotCurves() will succeed.

The core functionality of many functions has not been altered. Functions
to provide standardized regression coefficients, mean-centered, or
residual-centered regression coefficients have not changed much, although
the plots of them are improved. 


\section{Facilitating Collection of Summary Information}


\subsection{summarize: A replacement for summary}

When an R function provides output that is not suitable for a term
paper, the student must cobble together some code and assemble a customized
table. In my opinion, R's summary() function for data frames is not
adequate. It lacks summaries for diversity of observations. In addition,
the output is not formatted in a way that is conducive to the creation
of plots or tables. As a result, I offer the function summarize(),
which separates the numeric from factor variables and provides an
alphabetized summary. 

<<echo=F>>=
library(rockchalk)
@

Consider the results of applying summarize() to Fox's Chile data set
from the car package:

<<echo=T>>=
data(Chile)
(summChile <- summarize(Chile))
@

The result object is a list that includes a data frame for the numeric
variables, with named rows and (optionally) alphabetized columns,
as well as a separate report for each factor variable. Users who wish
to summarize only the numeric variables can run summarizeNumerics()
instead, while others who want to summarize only factors can run summarizeFactors().
The output from summarizeFactors() is a list of factor summaries. 

A companion function is centralValues(), which will provide only one
number for each variable in a data frame. For numerics, it returns
the mean, while for factor variables, it returns the mode. 

<<echo=T>>=
centralValues(Chile)
@


\subsection{Easier predictions and newdata objects}

Students struggle with R's predict() methods. The problem is in the
creation of a newdata object of interesting values of the predictors.
They are often befuddled when they need to create an object like myNewDF
in a command like \inputencoding{latin9}\lstinline!predict(m1, newdata = myNewDF)!\inputencoding{utf8}.
If the regression formula uses functions like ``as.numeric()'' or
``as.factor()'', its almost impossible for first-time R users to
get this right. In version 1.8, I believe I have solved the problem
entirely with the functions newdata() and predictOMatic(). 

Let's fit an example regression with the Chile data set from the car
package.

<<echo=T>>=
m1 <- lm(statusquo ~ age + income + population + region + sex, data=Chile)
@\\


\noindent The new data must include one column for each variable in
the model, and those variables must have exactly the same names as
were used in the regression formula. The default output of predictOMatic()
will cycle through the predictors, one by one.

<<echo=T>>=
m1pred <- predictOMatic(m1)
m1pred
@

The newdata() and predictOMatic() functions handle the details and
allow the user to adjust their requests to allow for very fine grained
control. Here are the key arguments for these functions.
\begin{enumerate}
\item divider. The name of an algorithm to select among observed values
for which predictions are to be calculated. These are the possibilities.

\begin{description}
\item [{``seq''}] an evenly spaced sequence of values from low to high
across the variable.
\item [{``quantile''}] quantile values that eminate from the center of
the variable ``outward''. 
\item [{``std.dev.''}] the mean plus or minus the standard deviation,
or 2 standard deviations, and so forth.
\item [{``table''}] when a variable has only a small set of possible
values, this selects the most frequently observed values.
\end{description}
\item n. The number of values to be selected.
\item predVals. Where the divider argument sets the default algorithm to
be used, the predVals argument can choose variables for focus and
select divider algorithms separately for the variables. It is also
allowed to declare particular values.
\end{enumerate}
The user can request that particular values of the predictors are
used, or can declare one of several algorithms for selection of focal
values. All of these details are managed by the argument called predVals,
which is described in the help pages (with plenty of examples). 

<<echo=T>>=
mypred2 <- predictOMatic(m1, predVals = c("age", "region"), n = 3)
mypred2
mypred3 <- predictOMatic(m1, predVals = c(age = "std.dev.", region = "table"), n = 3)
mypred3
mypred4 <- predictOMatic(m1, predVals = list(age = summChile$numerics[2:4, "age"], region = c("SA", "C","N")), n = 3)
mypred4
@

I've invested quite a bit of effort to make sure this works dependably
with complicated regression formulae. All formulae, such as y \textasciitilde{}
x1 + log(x2 + alpha) + ploy(x3, d) will just work. (In case one is
interested to know \emph{why} this works, the secret recipe is a new
function called model.data(). Under the hood, this required some hard
work, a frustrating chain of trial and error that is discussed in
the vignette \emph{Rchaeology}, which is distributed with this package).

The predictOMatic() function will work when the regression model provides
a predict function that can handle the newdata argument. If the regression
package does not provide such a function, the user should step back
and used the newdata function to create the examplar predictor data
frames and then calculate predicted values manually. The newdata()
function will set all variables to center values and then it will
create a ``mix and match'' combination of the ones that the user
asks for. This sequence shows ways to ask for various ``mix and match''
combinations of values from age and region. 

<<echo=T>>=
mynewdf <- newdata(m1, predVals = c("age","region"), n = 3)
mynewdf
@

<<echo=T>>=
mynewdf2 <- newdata(m1, predVals = list(age = "std.dev.", region = c("SA", "C","N")))
mynewdf2
@

<<echo=T>>=
mynewdf3 <- newdata(m1, predVals = list(age = c(20, 30, 40), region = c("SA", "C","N")))
mynewdf3
@\\
Of course, functions from rockchalk or any other R package can be
placed into the process for choosing focal values. 

<<echo=T>>=
mynewdf <- newdata(m1, predVals = list(age = getFocal(Chile$age, n = 3), region = getFocal(Chile$region, n = 3)))
mynewdf
@\\
The function getFocal() is a generic function; it will receive variables
of different types and ``do the right thing.'' By default, focal
values of numeric variables are quantile values. For factor values,
the most frequently observed values are selected. These are customizable,
as explained in the documentation. The newdata() output can be used
in a predict() function call as demonstrated above. 

It would be nice if every regression model's predicted values were
accompanied by 95\% confidence intervals. Models fit by lm() can supply
confidence intervals, but not glm(). At the current time, there are
many competing methods that might be used to calculate those intervals;
predict.glm() in R's stats package avoids the issue entirely by not
calculating intervals. In rockchalk-1.8, I crafted some code to calculate
confidence intervals for glm objects using the (admittedly crude)
Wald-based approximation. In the scale of the linear predictor, we
calculate a 95\% confidence interval, and then use the inverse link
function to transform that onto the space of the observed response.
In this example, I replicate an example that is an R classic, from
the help page of predict.glm. The reader will note that the output
includes a warning about the construction of the confidence interval.

<<echo=T>>=
df <- data.frame(ldose = rep(0:5, 2), sex = factor(rep(c("M", "F"), c(6, 6))), 
	SF.numdead = c(1, 4, 9, 13, 18, 20, 0, 2, 6, 10, 12, 16))      
df$SF.numalive <-  20 - df$SF.numdead
budworm.lg <- glm(cbind(SF.numdead, SF.numalive) ~ sex*ldose, data = df,  family = binomial)
predictOMatic(budworm.lg, predVals = c(ldose = "std.dev.", sex = "table"), interval = "confidence")  
@


\section{Better Regression Tables: Some outreg Examples.}

On May 8, 2006, Dave Armstrong, who was a political science PhD student
at University of Maryland, posted a code snippet in r-help that demonstrated
one way to use the ``cat'' function from R to write \LaTeX{} markup.
That gave me the idea to write a \LaTeX{} output scheme that would
help create some nice looking term and research papers. I wanted ``just
enough'' information, but not too much.

Since 2006, many new R packages have been introduced for the creation
of regression tables, but I still prefer to maintain outreg. I fight
to keep this simple, but have added some features in response to user
requests. In rockchalk-1.8, I have compromised with users who want
``more stars.'' Personally, I think the one-two-three stars for
p-values are silly, but they are used so widely that I now have incorporated
a user option to request various alphas. I've also made it easier
for users to request additional summary statistics in the bottom half
of the table.

In the following, I will demonstrate some tables for lm and glm fits
on a simulated data set. This new simulation function, genCorrelatedData2(),
is a convenient way to create multiple-predictor regression data sets
of arbitary size and complexity, allowing for interactions and nonlinearity. 

<<createdata1, echo=T>>=
set.seed(1234)
dat <- genCorrelatedData2(N = 100, means = c(0, 10, 0), sds = c(1, 1, 1), rho = c(0, 0, 0), 
  	stde = 5, beta = c(0, -3, 4, 0), verbose = FALSE)
@\\
That creates a new matrix with variables $x1,$ $x2$, $x3$, and
y. We run some linear regressions and then create a categorical output
variable for a logistic regression.

<<createdata1, echo=T>>=
m1 <- lm(y ~ x1 + x2, data = dat)
m2 <- lm(y ~ x2, data = dat)
m3 <- lm(y ~ x1 + x2 + x3, data = dat)
## Create categorical variant
myilogit <- function(x) exp(x)/(1 + exp(x))
dat$y3 <- rbinom(100, size = 1, p = myilogit(scale(dat$y)))
gm1 <- glm(y3 ~ x1 + x2, data = dat)
@

The outreg examples are offered in Tables \ref{tab:Tab1} through
\ref{tab:Combined-OLSGLM}. Table \ref{tab:Tab1} is the default output
for three models, obtained from \inputencoding{latin9}\lstinline!outreg(list(m1, m2, m3))!\inputencoding{utf8}.
On the other hand, users can pack output estimates into a single column
and ask for more stars, as illustrated in Table \ref{tab:Tab2}. .
In Table \ref{tab:Combined-OLSGLM}, observe that the linear and generalized
linear model output peacefully co-exist, side-by-side. This output
is, in my opinion, completely acceptable for inclusion in a presentation
or conference paper There are some warts in this output. Because the
model labels are not equal in length, the columns are not equally
sized. Because may \LaTeX{} distributions do not include the dcolumns
package, I've not aligned the coefficient estimates vertically as
some users might like.

\begin{table}
\caption{Default Outreg Table\label{tab:Tab1}}


<<outreg10, echo = F, results=tex>>=
outreg(list(m1, m2, m3))
@
\end{table}


\begin{table}
\caption{My Spread Out Regressions\label{tab:Tab2}}


<<outreg20, results=tex, echo=F>>=
outreg(list(m1, m3), tight = FALSE, modelLabels = c("The First Model with a Long Title", "Another Model"), alpha = c(0.05, 0.01, 0.001))
@
\end{table}


\begin{table}
\caption{Combined OLS and GLM Estimates\label{tab:Combined-OLSGLM}}


<<outreg70, results=tex, echo=F>>=
outreg(list(m1,gm1), modelLabels=c("OLS:y","GLM: Categorized y"))
@
\end{table}



\section{Plotting and Testing Interactions }


\subsection{Interaction in Linear Regression. }

One of the most fundamental skills in regression analysis is the interpretation
of interactive predictors. It is much easier for students to understand
the effect of an interaction if they can create a nice plot to show
how the predicted value depends on various values of a predictor.
The plotSlopes() function was introduced in 2010 when I was teaching
a large first-year graduate course (more than 50 students) and it
became apparent that about 20 percent of them would not be able to
manage the R coding required to draw several lines on a single plot.
Unfortunately, R's termplot() function will not draw regressions involving
interactions.

The rockchalk package has two functions to help with this, plotSlopes()
and plotCurves(). plotCurves() is more general, it can handle any
kind of formula that the user estimates. plotSlopes() is more limited,
it is only for lm objects. In return for that limitation, plotSlopes()
creates an output object which can be used to conduct post-hoc hypothesis
tests. 

\begin{figure}
<<ps05, fig=T, echo=F, include=F, height=5, width=6>>=
m1ps <- plotSlopes(m1, plotx = "x2", xlab = "x2 from model m1", interval = "confidence", opacity = 80, col = "red", ylim = c(20, 70))
@

\includegraphics[height=4in]{rockchalk-ps05}

\caption{plotSlopes Illustrated\label{fig:ps05}}
\end{figure}


At its most elementary level, plotSlopes() is a ``one step'' regression
line plotter. If the regression model includes more than one predictor,
then a single predictor is displayed on the horizontal axis and the
other predictors are set on their central values. A plot for the model
m1, that was illustrated above, is presented in Figure \ref{fig:ps05}.
In rockchalk-1.8, new arguments were added to allow the ``see though''
confidence region. The command to generate Figure \ref{fig:ps05}
was

<<echo=T, eval=F>>=
<<ps05>>
@

\noindent I've adjusted the color and opacity to illustrate the usage
of those arguments. The y range is adjusted to make a little extra
room for the legend. The plotSlopes() function is very flexible. All
of the label, color, and scale arguments of a plot function are also
available. The plotSlopes function also works well if the moderator
is a categorical variable. 

It is important to note that the output object, m1ps, has the information
necessary to re-create the plotted line in the form of a newdata data
frame. The first few lines in the newdata object are

<<>>=
m1ps$newdata[1:3, ]
@

<<ps09, fig=T, echo=F, height=9, width=6>>=
dat$y4 <- 1 + 0.1 * dat$x1 - 6.9 * dat$x2 + 0.5 * dat$x1*dat$x2 + 0.2 * dat$x3 + rnorm(100, m = 0, sd = 10)
m4 <- lm(y4 ~ x1*x2 + x3, data = dat)
@

\begin{figure}
<<ps10, fig=T, echo=F, height=9, width=6>>=
par(mfcol=c(2,1))
m4psa <- plotSlopes(m4, plotx = "x1", modx = "x2", xlab = "x1 is a fun plotx")
m4psb <- plotSlopes(m4, plotx = "x2", modx = "x1", modxVals = "std.dev.", xlab = "x2 is plotx", ylim = c(-100, 20))
par(mfcol=c(1,1))
@

\caption{plotSlopes Illustrated\label{fig:ps10}}
\end{figure}


Now, suppose we fit a regression with an interaction term. 

\inputencoding{latin9}\begin{lstlisting}
m4 <- lm(y4 ~ x1*x2 + x3, data = dat)
\end{lstlisting}
\inputencoding{utf8}

\noindent We then ask plotSlopes to draw the predicted values using
one numeric variable as the horizontal axis and values of another
variable (a moderator) are set at particular values. Either x1 or
x2 can be viewed as the ``moderator'' predictor, the one on which
the effect of the other depends. In version 1.8, the selection of
values of the moderator was generalized, so that the user can specify
either a function that selects values, or a vector of values, or the
name of an algorithm. The default algorithm will choose quantile values,
but Figure \ref{fig:ps10} demonstrates also the ``std.dev.'' divider
algorithm. The code to produce that figure was

<<eval=F, echo=T>>=
<<ps10>>
@

When modx is a numeric variable, then some particular values must
be selected for calculation of predicted value lines. The modxVals
argument is used to either specify moderator values or an algorithm
to select focal values. By default, three hypothetical values of plotx
are selected (the quantiles 25\%, 50\%, and 75\%). 

If modx is a factor variable, then the most frequently observed scores
will be selected for consideration. The default display will include
the regression line as well as color-coded points for the subgroups
represented by values of the moderator.

\noindent 
\begin{figure}
<<echo=F>>=
fourCat <- gl(4,25, labels=c("East","West","South", "Midwest"))
dat$x4 <- sample(fourCat, 100, replace = TRUE)
dat$y5 <- 1 + 0.1 * dat$x1 + contrasts(dat$x4)[dat$x4, ] %*% c(-1,1,2) + rnorm(100,0, sd=10)
m5 <- lm (y5 ~ x1*x4 + x3, data=dat)
@

<<ps20, fig=T, echo=F>>=
m5psa <- plotSlopes(m5, plotx = "x1", modx = "x4", xlab = "x1 is a Continuous Predictor", xlim = magRange(dat$x1, c(1.2,1)))
@

\caption{plotSlopes with a Categorical Moderator\label{fig:ps20}}
\end{figure}


\noindent 
\begin{figure}
<<ps21, fig=T, echo=F>>=
m5psb <- plotSlopes(m5, plotx = "x1", modx = "x4", modxVals = c("West","East"), xlab = "x1 is a Continuous Predictor", xlim=magRange(dat$x1, c(1.2,1)), interval = "conf")
@

\caption{plotSlopes: the interval argument\label{fig:ps21}}
\end{figure}
Suppose we have a four-valued categorical variable, ``West'',''Midwest'',
``South'', and ``East''. If that variable is used in an interaction
in the regression model, then the plotSlopes output will include four
lines, one for each region. For example, consider Figure \ref{fig:ps20},
which is created by

\noindent <<eval=F>>=
<<ps20>>
@

\noindent The categorical variable is x4. 

It is possible to superimpose confidence intervals for many subgroups,
but sometimes these plots start to look a little bit ``busy''. The
mixing of shades in overlapping intervals may help with that problem.
A plot that focuses on just two subgroups is presented in Figure \ref{fig:ps21},
which is produced by 

\noindent <<eval=F>>=
<<ps21>>
@ 

In rockchalk version 1.8, I've exerted quite a bit of effort to make
sure that colors are chosen consistently when users remove or insert
groups in these plots. The same value of the moderator should always
be plotted in the same way--the line, points, and interval colors
should not change. Note, for example, in Figures \ref{fig:ps20} and
\ref{fig:ps21}, the line for East is black in both plots, while the
line for West is red in both. 


\subsection{testSlopes, a companion of plotSlopes}

In psychology, methodologists have recommended the analysis of ``simple
slopes'' to depict the effect of several variables in a 2 dimensional
plot. This is most often of interest in the analysis of regression
models with interactive terms. 

Aiken and West (and later Cohen, Cohen, West, and Aiken) propose using
the t test to find out if the effect of the ``plotx'' variable is
statistically significantly different from zero for each particular
value of the moderator variable. The user should first run plotSlopes,
and then submit the output object to testSlopes. The usual case would
be the following:

\inputencoding{latin9}\begin{lstlisting}
m4 <- lm (y ~ x1*x2 + x3, data = dat)
m4ps <- plotSlopes(m4, plotx = "x1", modx ="x2", xlab = "x1 is a Continuous Predictor")
testSlopes(m4ps)
\end{lstlisting}
\inputencoding{utf8}

\noindent The output from that testSlopes usage is illustrated in
Figure \ref{fig:ts10}.

\begin{figure}
<<ts10, fig=T, echo=T, height=6, width=6>>=
m4psats <- testSlopes(m4psa)
plot(m4psats)
@

\caption{testSlopes for an Interactive Model\label{fig:ts10}}
\end{figure}


The hypothesis tests reported by testSlopes should be understood as
follows. Each of the lines in the output from plotSlopes, say Figure
\ref{fig:ps10}, can be tested to find out if its ``simple slope''
is different from zero. The tests calculated by testSlopes represent
the null hypothesis that 
\begin{equation}
H_{0}:0=\hat{b}_{simple\, slope}=\hat{b}_{plotx}+b_{plotx\cdot modx}modx
\end{equation}


\noindent where $modx$ is the moderator variable and $plotx$ is
plotted on the horizontal axis in the plotSlopes output. 

Following a suggestion of Preacher, Curran, and Bauer (2006), the
testSlopes function also tries to calculate the Johnson-Neyman (1936)
interpretation of the same test. It presents a plot, as illustrated
in Figure \ref{fig:ts10}. The J-N test would have us ask, ``for
which values of the moderator would the value $\hat{b}_{simple\, slope}$
be statistically significantly different from zero?'' The J-N calculation
requires the solution an equation that is quadratic in the value of
the moderator variable, $modx$. The interval of values of $modx$
associated with a statistically significant effect of $plotx$ on
the outcome is determined from the computation of a T statistic for
$\hat{b}_{simple\, slope}$. The J-N interval is the set of values
of $modx$ for which the following holds:
\begin{equation}
\hat{t}=\frac{\hat{b}_{simple\, slope}}{std.err(\hat{b}_{simple\, slope})}=\frac{\hat{b}_{simple\, slope}}{\sqrt{\widehat{Var(\hat{b}_{plotx})}+modx^{2}\widehat{Var(\hat{b}_{plotx\cdot modx})}+2modx\widehat{Cov(\hat{b}_{plotx},\hat{b}_{plotx\cdot modx})}}}\geq T_{\frac{\alpha}{2},df}
\end{equation}


\noindent Suppose there are two real roots, $root1$ and $root2$.
The values of $modx$ for which the slope is statistically significant
may be a compact interval, $[root1,root2]$, or it may two open intervals,
$(-\infty,root1]$ and $[root2,\infty)$. 

\noindent The J-N interpretation is most useful when the moderator
is a continuous variable and the result specifies an interval inside
the range of the moderator. In quite a few cases, the J-N interval
is outside the observed range of the moderator, which makes it either
difficult to interpret or irrelevant.


\subsection{plotCurves for nonlinear predictor formulae}

plotCurves generalizes the plotting capability of plotSlopes. plotCurves
should be able to handle any regression formulas that include nonlinear
transformations. Models that have polynomials or terms that are logged
(or otherwise transformed) can be plotted. In that sense, plotCurves
is rather similar to R's own termplot function. The difference is
that plotCurves allows for moderator variables, which implies that
one can draw several different curves to represent separate groups.

Suppose a dependent variable y5 is created according to a nonlinear
process.
\begin{equation}
y5_{i}=-3x1_{i}+1.5*log(x2)+1.1x2_{i}+2.2x1_{i}\times x2_{i}+e_{i}
\end{equation}


<<echo=F,include=F>>=
dat$y5 <- with(dat, -3*x1 + 3.5*log(x2) + 2.1*x2 + 2.2 *x1 * x2 + 20*rnorm(100)) 
@

\begin{figure}
<<pcps20, fig=T, height=6, width=6>>=
m6 <- lm (y5 ~ log(x2*x2) + x1 * x2, data=dat)
plotCurves(m6, plotx="x2", modx="x1")
@

\caption{plotCurves\label{fig:pcps20}}
\end{figure}



\subsection{plotPlane}

The persp function in R works well, but its interface is too complicated
for most elementary and intermediate R users. To facilitate its use
for regression users, the plotPlane is offered.

The plotPlane function offers a visualization of the mutual effect
of two predictors in m4. See Figure \ref{fig:pp100} for the plot
created by

\noindent \inputencoding{latin9}
\begin{lstlisting}
plotPlane(m4, plotx1="x1", plotx2="x2")
\end{lstlisting}
\inputencoding{utf8}

\begin{figure}
<<pp100, fig=T>>=
p100 <- plotPlane(m4, plotx1 = "x1", plotx2 = "x2")
@

\caption{plotPlane for the Interactive Model\label{fig:pp100}}
\end{figure}


plotPlane is designed to work like plotCurves, to tolerate nonlinear
components in the regression formula. As illustrated in Figure \ref{fig:pcps10},
plotPlane allows the depiction of a 3 dimensional curving plane that
``sits'' in the cloud of data points. The variables that are not
explicitly pictured in the plotPlane figure are set to central reference
values. As illustrated in Figure \ref{fig:pcps20}, plotCurves is
a 2 dimensional depiction of the same model. 

\begin{figure}
<<pcps10, fig=T, height=4, width=4>>=
plotPlane(m6, plotx1 = "x1", plotx2 = "x2")
@

<<results=tex>>=
outreg(m6, tight = FALSE)
@

\caption{plotPlane\label{fig:pcps10}}
\end{figure}


At some point in the future, the ability to make plotSlopes and plotPlane
work together will be introduced. The user will be able to see how
the two and three dimensional graphs relate to each other. A preliminary
rendering of what that might look like is presented in Figure \ref{fig:pp110}.
It is as if we can ``press the plane down'' into the 2-D slopes
plot, or the 2-D simple slopes can be depicted in the 3 dimensional
plane.

\begin{figure}
<<pp110, fig=T, echo=F, width=5, height=4>>=
m6ps <- plotSlopes(m6, plotx = "x1", modx = "x2", xlab = "Continuous Predictor", ylim = c(-25, 105))
@
<<pp111, fig = T, echo = F, height = 5>>=
p110 <- plotPlane(m6, plotx1 = "x1", plotx2 = "x2", x1lab = "Continuous Predictor", phi = 30)
addLines(from = m6ps, to = p110, col = m6ps$col)
#for(j in unique(m6ps$newdata$x2)){
#    subdat <- m6ps$newdata[m6ps$newdata$x2==j,]
#    lines(trans3d(subdat$x1, subdat$x2, subdat$fit, pmat = p110$res), col = "red", lwd = 3)
#}
@

\caption{Making plotSlopes and plotPlane work Together\label{fig:pp110}}
\end{figure}



\section{Standardized, Mean-Centered, and Residual-Centered Regressions }


\subsection{Standardized regression}

Many of us learned to conduct regression analysis with SPSS, which
(historically, at least) reported both the ordinary regression coefficients
as well as a column of coefficients obtained from a regression in
which each of the predictors in the design matrix had been ``standardized.''
That is to say, each variable, for example $x1_{i}$, was replaced
by an estimated $Z-score:$ $(x1_{i}-\overline{x1})/std.dev.(x1_{i}$).
A regression fitted with those standardized variables is said to produce
``standardized coefficients.'' These standardized coefficients,
dubbed ``beta weights'' in common parlance, were thought to set
different kinds of variables onto a common metric. While this idea
appears to have been in error (see, for example, King 1986), it still
is of interest to many scholars who want to standardize their variables
in order to compare them more easily. 

The function standardize was included in rockchalk to facilitate lectures
about what a researcher ought not do. standardize performs the complete,
mindless standardization of all predictors, no matter whether they
are categorical, interaction terms, or transformed values (such as
logs). Each column of the design matrix is scaled to a new variable
with mean 0 and standard deviation 1. The input to standardize should
be a fitted regression model. For example:

<<>>=
m4 <- lm (y4 ~ x1 * x2, data = dat)
m4s <- standardize(m4)
@

It does seem odd to me that a person would actually want a standardized
regression of that sort, and the commentary included with the summary
method for the standardized regression object probably makes that
clear.

<<>>=
summary(m4s)
@

\begin{table}
\caption{Comparing Ordinary and Standardized Regression\label{tab:stdreg10}}


<<stdreg10, results=tex, echo=F>>=
outreg(list(m4, m4s), tight = F, modelLabels = c("Not Standardized","Standardized"))
@
\end{table}



\subsection{Mean-centered Interaction Models}

Sometimes people will fit a model like this
\begin{equation}
y_{i}=b_{o}+b_{1}x1_{i}+b_{2}x2_{i}+e_{i}
\end{equation}


\noindent and then wonder, ``is there an interaction between $x1_{i}$
and $x2_{i}$?'' The natural inclination is to run this model, 

\inputencoding{latin9}\begin{lstlisting}
m1 <- lm(y ~ x1*x2)
\end{lstlisting}
\inputencoding{utf8}

\noindent or its equivalent

\inputencoding{latin9}\begin{lstlisting}
m2 <- lm(y ~ x1 + x2 + x1:x2)
\end{lstlisting}
\inputencoding{utf8}

For a variety of reasons, researchers have been advised that they
should not run the ordinary interaction model. Instead, they should
``mean center'' the variables $x1$ and $x2$ before entering them
into the regression model. That is, they should replace $x1_{i}$
with $(x1_{i}-\overline{x1})$ and $x2_{i}$ with $(x2_{i}-\overline{x2})$,
so that the fitted model is actually
\begin{equation}
y_{i}=b_{o}+b_{1}(x1_{i}-\overline{x1})+b_{2}(x2_{i}-\overline{x2})+b_{3}(x1_{i}-\overline{x1})(x2_{i}-\overline{x2})+e_{i}
\end{equation}


This is easy enough to do in R, but it can be tedious to center the
variables and then run the model. To make it easier for users to compare
the results of the ``ordinary interaction'' and the ``mean centered''
model, this package includes a function meanCenter. meanCenter will
receive a model, scan it for interaction terms, and then center the
variables that are involved in interactions. It is used as follows.
First, fit any regression, such as m4 above, the same one with which
the standardize function was demonstrated. Pass the output object
to the meanCenter function.

<<>>=
m4mc <- meanCenter(m4)
summary(m4mc)
@

The default settings for meanCenter cause it to center only the variables
involved in an interaction, and it leaves the others unchanged. If
the user wants all of the numeric predictors to be mean-centered,
the usage would be

\inputencoding{latin9}\begin{lstlisting}
m4mc <- meanCenter(m4, centerOnlyInteractors = FALSE)
summary(m1mc4)
\end{lstlisting}
\inputencoding{utf8}

By default, it does not standardize while centering (but the user
can request standardization). Users who want to standardize the variables
that are centered can use the argument standardize=TRUE. The option
centerDV causes the dependent variable to be centered as well.


\subsection{Residual-centered Models}

Residual-centering is another adjustment that has been recommended
for models that include interactions or squared terms. Like mean-centering,
it is recommended mainly as a way to ameliorate multicollinearity. 

I think of residual-centering as follows. Suppose we fit the linear
model, with no interaction

\begin{equation}
y=c_{0}+c_{1}x1+c_{2}x2+e{}_{i}.\label{eq:rc20}
\end{equation}


\noindent Suppose that those parameter estimates, $\hat{c}_{1}$,
$\hat{c}_{2}$, are the ``right ones''. We want to estimate the
interactive model, 
\begin{equation}
y_{i}=b_{o}+b_{1}x1_{i}+b_{2}x2_{i}+b_{3}x1_{i}\times x2_{i}+e_{i},
\end{equation}
but if we estimate that, it will ``ruin'' our estimates for the
effects of $x1$ and $x2$. So we proceed by constraining the fitted
coefficients in the interactive model so that the main effects remain
the same. That is to say, require that the parameter estimates of
$x1$ and $x2$ must match match estimates from equation \ref{eq:rc20}.
Effectively, $\hat{b}_{1}=\hat{c}_{1}$ and $\hat{b}_{2}=\hat{c}_{2}$.

How can this be done in a convenient, practical way? The answer: use
``residual-centering.'' First, estimate the following regression,
in which the left hand side is the interaction product term:
\begin{equation}
(x1_{i}\times x2_{i})=d_{0}+d_{1}x1_{i}+d_{2}x2+u_{i}\label{eq:residCentered}
\end{equation}


The residuals from that regression are, by definition, orthogonal
to both $x1$ and $x2$. Call those fitted residuals $\widehat{u_{i}}$.
We fit the interactive model using $\widehat{u}_{i}$ in place actual
product term $(x1_{i}\times x2_{i})$. 

\begin{equation}
y_{i}=b_{0}+b_{1}x1_{i}+b_{2}x2_{i}+b3\widehat{u_{i}}+e_{i},\label{eq:rc10-1}
\end{equation}


\noindent In essence, we have taken the interaction $(x1_{i}\times x2_{i})$,
and purged it of its parts that are linearly related to $x1_{i}$
and $x2_{i}$ separately. 

The rockchalk function residualCenter handles this for the user. Like
meanCenter, the user has to fit an interactive model first, and the
result object is passed to residualCenter like so:

<<>>=
m4rc <- residualCenter(m4)
summary(m4rc)
@

The objects created by residualCenter are assigned the class ``rcreg''
and the package includes summary, print, and predict methods for these
objects. In the regression output, the residual-centered interaction
term is labeled as $(x1Xx2)$. 


\subsection{Why are we bothering with mean-centering and residual-centering in
the first place?}

In the long run, think the correct answer will be, ``we were mistaken.''
Nevertheless, the advice that one ought to mean-center or residual-center
in regression analysis has become quite widely established. The primary
advocates of ``mean-centering'' have been Aiken and West (1991),
who integrated that advice into the very widely used regression textbook,
\emph{Applied Multiple Regression/Correlation for the Behavioral Sciences}
(Cohen, et. al , 2002). The advice that one ought to mean-center the
predictors has been picked up in other fields. One statistics text
for biologists notes, ``We support the recommendation of Aiken \&
West (1991) and others that multiple regression with interaction terms
should be fitted to data with centered predictor values'' (Quinn
and Keough, 2002, Chapter 6). 

In order to understand how mean-centering came to seem like a ``magic
bullet,'' it is necessary to re-trace some steps to find out how
we arrived in our current situation. For this example, I used the
function genCorrelatedData in rockchalk. The ``true model'' from
which the data is produced is
\begin{equation}
y_{i}=2+0.1x1_{i}+0.1x2_{i}+0.2\cdot(x1_{i}\times x2_{i})+e_{i},
\end{equation}


\noindent where $e_{i}\sim N(0,300^{2})$ and $\rho_{x1,x2}=0.4$.

Virtually everybody who has experimented with regression has had the
``what the heck happened to my predictors?'' experience. Please
consider the Table \ref{tab:meancenter10-1}. In the first column,
we have the ordinary linear specification

\inputencoding{latin9}\begin{lstlisting}
lm(y ~ x1 + x2, data=dat2).
\end{lstlisting}
\inputencoding{utf8}

\noindent The coefficients of $x1$ and $x2$ appear to be ``statistically
significant,'' a very gratifying regression indeed. It appears we
might have found something!

Unable to leave well enough alone, the researcher wonders, ``is there
an interaction between $x1$ and $x2$?'' The second column in Table
\ref{tab:meancenter10-1} summarizes the regression that includes
an interaction term. That interaction model, which adds the product
variable $x1\times x2$, is estimated in R with

\inputencoding{latin9}\begin{lstlisting}
lm(y ~ x1 * x2, data=dat2)
\end{lstlisting}
\inputencoding{utf8}

\noindent A quick scan of column two usually lead to the ``what the
heck?'' or ``Holy Cow!'' response. The regression went to hell!
Neither of the key variables, $x1$ nor $x2$, is ``statistically
significant'' any more. While the coefficients for the variables
$x1$ and $x2$ did seem to be substantial in the first model, the
introduction of the interactive effect appears to ruin the whole thing.
What should be done when adding a product term seems to ``ruin''
a regression model? 

<<echo=F, include=F>>=
dat2 <- genCorrelatedData(N=400, rho=.4, stde=300, beta=c(2,0.1,0.1,0.2))

m6linear <- lm (y ~ x1 + x2, data=dat2)
m6int <- lm (y ~ x1 * x2, data=dat2)
m6mc <- meanCenter(m6int)
m6rc <- residualCenter(m6int)
@

\begin{table}
\caption{Comparing Regressions\label{tab:meancenter10-1}}


<<mcenter10, results=tex, echo=F>>=
outreg(list(m6linear, m6int, m6mc, m6rc), tight=F, modelLabels=c("Linear", "Interaction","Mean-centered","Residual-centered"))
@
\end{table}


Cohen, et al. refer to the apparent instability of the coefficients
as a reflection of ``inessential collinearity'' among the predictors,
due to the fact that $x1$ and $x2$ are correlated with the new term,
$x1\times x2$. They advised their readers to ``mean center'' their
predictors, to subtract the mean of each predictor from the observed
model and run the regression again. 

Mean-centering seems to help. The result of the meanCenter function
is displayed in the third column of Table \ref{tab:meancenter10-1}.
It appears that the estimates for the slopes are ``significant again''
and we have ``solved'' the problem of inessential collinearity. 

The solution, however, is simply an illusion. Technical rebuttals
have been published (Kromrey, J. D., \& Foster-Johnson, L. , 1998;
Echambadi and Hess, 2007), but applied researchers continue to use
the practice. The argument against mean-centering is really quite
simple. It has no effect. There is no benefit. The ordinary model
and the mean-centered models are actually exactly the same in every
important way. The technical critiques have focused on the multicollinearity
issue, but they leave open the possibility that mean-centering may
facilitate interpretation of the estimates. The presentation here
should convince the reader that even the interpretation is not facilitated
by mean-centering.

The first hint of trouble is in the fact that the coefficient of the
interactive effect in columns 2 and 3 is identical. Those coefficients
are the same because they are estimates of a constant, the cross partial
derivative $\partial^{2}y/\partial x1\partial x2$. That is to say,
when the different models try to estimate the same coefficient, they
get the same result. Note as well that the root mean square and $R^{2}$
estimates are identical. Is it possible that the mean-centered regression
could really be ``better'' if its fit statistics are not altered?

The models only appear different because we sometimes forget that
we are studying a nonlinear problem when the regression model includes
interactions. To assist in the visualization of this situation, we
can use some functions in rockchalk. In Figure \ref{fig:mcenter30},
we see that the ordinary interaction and mean-centered models produce
identical predicted values! 

\begin{figure}
<<mcenter50, fig=T,echo=FALSE, height=5, width=7>>=
op <- par(no.readonly = TRUE)
par(mfcol=c(2,2))
par(mar=c(2,2,2,1))
plotPlane(m6linear, plotx1="x1", plotx2="x2", plotPoints=FALSE, main="Linear", ticktype="detailed")
plotPlane(m6int, plotx1="x1", plotx2="x2", plotPoints=FALSE, main="Interaction: Not Centered", ticktype="detailed")
plotPlane(m6mc, plotx1="x1c", plotx2="x2c", plotPoints=FALSE, main="Mean-centered", ticktype="detailed")
plotPlane(m6rc, plotx1="x1", plotx2="x2", plotPoints=FALSE, main="Residual-centered", ticktype="detailed")
par(op)
@

\caption{Predicted Planes from Centered and Uncentered Fits Identical\label{fig:mcenter30}}
\end{figure}


The curves in Figure \ref{fig:mcenter30} are identical. With the
original, non-transformed data, the vertical (y) axis is positioned
at the front-left edge of the graph, while the centered one re-positions
the y axis into the center of the graph. The coefficient estimates
from a regression model on a nonlinear surface depend on the location
of the y axis. At some points, the estimates will be large, at some
points they will be small. Mean-centering may accidentally reposition
the axis to a location that has ``better'' (bigger?) point estimates.
The estimated standard errors, of course, also change as we move the
y axis about. In the middle of the data, the point estimates of the
standard errors are generally smaller than they are when the axis
is positioned on the periphery of the data. It can be shown that,
from either fitted model, the estimated slopes and standard errors
at any given point in the data are exactly the same. Included with
the rockchalk package, in the examples folder, one can find a file
called ``residualCentering.R'' that walks through this argument
step by step. 

If mean-centering does not help, perhaps residual-centering will address
the problem. Residual-centering is offered as a new, improved sort
of mean-centering. Where mean-centering seems to reduce the damage
done by inessential collinearity, residual-centering completely eliminates
it. Because the residual-centered interaction variable is, by definition,
completely uncorrelated with the other variables in the model, the
problem of collinearity seems to be completely solved. 

The fourth column in Table \ref{tab:meancenter10-1} presents the
residual centered results. The parameter estimates are a little larger,
the standard errors are a bit smaller. In the way that applied researchers
usually look at situations like this, it is a ``better'' model. 

And, yet again, our high hopes are dashed. In the end, we will find
out that the residual-centered model is completely equivalent to the
ordinary interaction model and the mean-centered model. My effort
to visualize this was initially frustrated by the difficulty of writing
a predict function that worked for arbitrary new data objects, but
once that was finished, the result became completely clear. The predicted
values of the ordinary interactive model, the mean-centered model,
and the residual-centered models are exactly the same. Perhaps the
most persuasive case is found in Figure \ref{fig:rcenter40}.

\begin{figure}
<<rcenter40, fig=T,echo=FALSE, height=5, width=7>>=
dat3 <- centerNumerics(dat2)
##m6mcpred <- fitted(m6mc) ##
m6mcpred <- predict(m6mc, newdata=dat3)
##m6rcpred <- fitted(m6rc) ##
m6rcpred <- predict(m6rc, newdata=dat3)
##m6intpred <- fitted(m6int) ##
m6intpred <- predict(m6int, newdata=dat3)
op <- par(no.readonly = TRUE)
par(mfcol=c(1,2))
##plot(fitted(m6rc), predict(m6rc, newdata=dat3))
##plot(fitted(m6mc), predict(m6mc, newdata=dat3))
plot(m6intpred, m6rcpred, main="", xlab="Predictions of Uncentered Interaction", ylab="Residual-centered Predictions")
predcor <- round(cor(m6intpred, m6rcpred),3)
legend("topleft", legend=c(paste("Correlation=", predcor)))
plot(m6mcpred, m6rcpred, main="", xlab="Mean-centered Predictions", ylab = "Residual-centered Predictions")
predcor <- round(cor(m6mcpred, m6rcpred),3)
legend("topleft", legend=c(paste("Correlation=", predcor)))
par(op)
@

\caption{Predicted Values of Mean and Residual-centered Models\label{fig:rcenter40}}
\end{figure}


The conclusion is this. One can code a nonlinear model in various
ways, all of which are theoretically and analytically identical. There
are superficial differences in the estimates of the coefficients of
the various specifications, but these differences are understandable
in light of the changes in the design matrix. The connection between
the observed values of the predictors and the predicted values remains
the same in all of these specifications.

Why do the coefficients differ if the models are actually the same?
Recall that we are estimating the slopes of a curving plane, and so
estimates of the marginal effects of $x1$ and $x2$ will depend on
the point at which we are calculating the slopes. Mean-centering and
residual-centering are different methods for re-positioning the $y$
axis. The interactive model has a constant mixed partial derivative,
so the estimate of the interaction coefficient is not affected by
the position of the y axis. The other coefficients, however, do change. 

It is possible to translate between the estimates of any one of these
fitted models and the estimates of another. The derivation proceeds
as follows. The ordinary model is
\begin{equation}
y_{i}=b_{0}+b_{1}x1_{i}+b_{2}x2_{i}+b3(x1_{i}\times x2_{i})+e1_{i}\label{eq:int}
\end{equation}


The mean-centered model is 
\begin{equation}
y_{i}=c_{0}+c_{1}(x1_{i}-\overline{x1})+c_{2}(x2_{i}-\overline{x2})+c_{3}(x1_{i}-\overline{x1})\times(x2_{i}-\overline{x2})+e2_{i}\label{eq:mc1}
\end{equation}


In order to compare with equation \ref{eq:int}, we would re-arrange
like so

\begin{equation}
y_{i}=c_{0}+c_{1}(x1_{i})-c_{1}\overline{x1}+c_{2}(x2_{i})-c_{2}\overline{x2}+c_{3}(x1_{i}x2_{i}+\overline{x1}\overline{x2}-\overline{x1}x2_{i}-\overline{x2}x1_{i})+e2_{i}
\end{equation}


\begin{equation}
y_{i}=c_{0}+c_{1}(x1_{i})-c_{1}\overline{x1}+c_{2}(x2_{i})-c_{2}\overline{x2}+c_{3}(x1_{i}x2_{i})+c_{3}\overline{x1}\overline{x2}-c_{3}\overline{x1}x2_{i}-c_{3}\overline{x2}x1_{i})+e2_{i}
\end{equation}


\begin{equation}
y_{i}=\{c_{0}-c_{1}\overline{x1}-c_{2}\overline{x2}+c_{3}\overline{x1}\overline{x2}\}+\{c_{1}-c_{3}\overline{x2}\}x1_{i}+\{c_{2}-c_{3}\overline{x1}\}x2_{i}+c_{3}(x1_{i}x2_{i})+e2_{i}\label{eq:mc3}
\end{equation}


One can then compare the parameter estimates from equations \ref{eq:int}
and \ref{eq:mc3} in order to understand the observed changes in fitted
coefficients after changing from the ordinary to the mean-centered
coding. Both \ref{eq:int} and \ref{eq:mc3} include a single parameter
times $(x1_{i}x2_{i}),$ leading one to expect that the estimate $\hat{b}_{3}$
should be equal to the estimate of $\hat{c}_{3}$ (and they are, as
we have found). Less obviously, one can use the fitted coefficients
from either model to deduce the fitted coefficients from the other.
The following equalities describe that relationship.
\begin{eqnarray}
\hat{b}_{0} & = & \hat{c}_{0}-\hat{c}_{1}\overline{x1}-\hat{c}_{2}\overline{x2}+\hat{c}_{3}\overline{x1}\overline{x2}\\
\hat{b}_{1} & = & \hat{c}_{1}-\hat{c}_{3}\overline{x2}\\
\hat{b}_{2} & = & \hat{c}_{2}-\hat{c}_{3}\overline{x1}\\
\hat{b}_{3} & = & \hat{c}_{3}
\end{eqnarray}
The estimated fit of equation \ref{eq:mc1} would provide estimated
coefficients $\hat{c}_{j}$, $j=0,...,3$, which would then be used
to calculate the estimates from the noncentered model.

The estimation of the residual centered model requires two steps.
First, estimate a regression 
\begin{equation}
(x1_{i}\times x2_{i})=d_{0}+d_{1}x1_{i}+d_{2}x2_{i}+u_{i}
\end{equation}


\noindent from which the predicted value can be calculated:
\begin{equation}
\widehat{(x1_{i}\times x2_{i})}=\hat{d}_{0}+\hat{d}_{1}x1_{i}+\hat{d}_{2}x2_{i}.
\end{equation}


\noindent The residual of that regression 
\begin{equation}
\widehat{u}_{i}=(x1_{i}\times x2_{i})-\widehat{(x1_{i}\times x2_{i})}
\end{equation}


\noindent is used as the ``residual-centered'' predictor in place
of $(x1_{i}\times x2_{i})$ in equation \ref{eq:int}. 

\begin{equation}
y_{i}=h_{0}+h_{1}x1_{i}+h_{2}x2_{i}+h_{3}\{x1_{i}\times x2_{i}-\widehat{x1_{i}\times x2_{i}}\}+e3_{i}\label{eq:rc1}
\end{equation}


Replacing $\widehat{x1_{i}\times x2_{i}}$ with $\hat{d}_{0}+\hat{d}_{1}x1_{i}+\hat{d}_{2}x2_{i}$,
\ref{eq:rc1} becomes

\begin{eqnarray}
y_{i} & = & h_{0}+h_{1}x1_{i}+h_{2}x2_{i}+h_{3}\{x1_{i}\times x2_{i}-\hat{d}_{0}-\hat{d}_{1}x1_{i}-\hat{d}_{2}x2_{i}\}+e3_{i}\\
 & = & h_{0}+h_{1}x1_{i}+h_{2}x2_{i}+h_{3}\{x1_{i}\times x2_{i}\}-h_{3}\hat{d}_{0}-h_{3}\hat{d}_{1}x1_{i}-h_{3}\hat{d}_{2}x2_{i}\}+e3_{i}\\
 &  & \{h_{0}-h_{3}\hat{d}_{0}\}+\{h_{1}-h_{3}\hat{d}_{1}\}x1_{i}+\{h_{2}-h_{3}\hat{d}_{2}\}x2_{i}+h_{3}\{x1_{i}\times x2_{i}\}+e3_{i}
\end{eqnarray}


As in the previous comparison of models, we can translate coefficient
estimates between the ordinary specification and the residual-centered
model. The coefficient estimated for the product term, $\hat{h}_{3}$,
should be equal to $\hat{b}_{3}$ and $\hat{c}_{3}$ (and it is!).
If we fit the residual centered model, \ref{eq:rc1}, we can re-generate
the coefficients of the other models like so: 
\begin{eqnarray}
b_{0} & =\hat{c}_{0}-\hat{c}_{1}\overline{x1}-\hat{c}_{2}\overline{x2}+\hat{c}_{3}\overline{x1}\overline{x2}= & h_{0}-h_{3}\hat{d}_{0}\\
b_{1} & =\hat{c}_{1}-\hat{c}_{3}\overline{x2}= & h_{1}-h_{3}\hat{d}_{1}\\
b_{2} & =\hat{c}_{2}-\hat{c}_{3}\overline{x1}= & h_{2}-h_{3}\hat{d}_{2}
\end{eqnarray}


From the preceding, it should be clear enough that the three models
are equivalent, in the sense that the parameter estimates (parameters,
predicted values, and so forth) from any one can be translated into
the terminology of the other.
\begin{thebibliography}{1}
\bibitem{key-1}Aiken, L. S., \& West, S. G. (1991). \emph{Multiple
Regression: Testing and Interpreting Interactions}. Newbury Park,
Calif: Sage Publications.

\bibitem{key-2}Cohen, J., Cohen, P., West, S. G., \& Aiken, L. S.
(2002). \emph{Applied Multiple Regression/Correlation Analysis for
the Behavioral Sciences} (Third.). Routledge Academic.

\bibitem{key-3}Echambadi, R., \& Hess, J. D. (2007). Mean-Centering
Does Not Alleviate Collinearity Problems in Moderated Multiple Regression
\emph{Models. Marketing Science}, 26(3), 438–445. doi: 10.1287/mksc.1060.0263

\bibitem{key-4}Kromrey, J. D., \& Foster-Johnson, L. (1998). Mean
Centering in Moderated Multiple Regression: Much Ado about Nothing.
\emph{Educational and Psychological Measurement}, 58(1), 42 –67. doi:10.1177/0013164498058001005

\bibitem{key-7}Little, T. D., Bovaird, J. A., and Widaman, K. F.
(2006). On the Merits of Orthogonalizing Powered and Product Terms:
Implications for Modeling Interactions Among Latent Variables. \emph{Structural
Equation Modeling}, 13(4), 497-519.

\bibitem{key-6}Preacher, K. J., Curran, P. J., \& Bauer, D. J. (2006).
Computational Tools for Probing Interactions in Multiple Linear Regression,
Multilevel Modeling, and Latent Curve Analysis. \emph{Journal of Educational
and Behavioral Statistics}, 31(4), 437–448.

\bibitem{key-5}Quinn, G.P, and Keough, Michael J. (2002) Experimental
Design and Data Analysis for Biologists. Cambridge University Press.The
primary advocates of ``mean-centering'' have been Aiken and West
(1991), who then integrated that advice into the very widely used
regression textbook, \emph{Applied Multiple Regression/Correlation
for the Behavioral Sciences} (Cohen, et. al , 2002). \end{thebibliography}

\end{document}
